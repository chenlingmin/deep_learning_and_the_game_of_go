# tag::alphago_sl_data[]
from multiprocessing import freeze_support

from dlgo.data.parallel_processor import GoDataProcessor
from dlgo.encoders.alphago import AlphaGoEncoder
from dlgo.agent.predict import DeepLearningAgent
from dlgo.networks.alphago import alphago_model

from keras.callbacks import ModelCheckpoint
import h5py


def main():
    rows, cols = 19, 19
    num_classes = rows * cols
    num_games = 1000

    encoder = AlphaGoEncoder()
    processor = GoDataProcessor(encoder=encoder.name())
    generator = processor.load_go_data('train', num_games, use_generator=True)
    test_generator = processor.load_go_data('test', num_games, use_generator=True)
    # end::alphago_sl_data[]

    # tag::alphago_sl_model[]
    input_shape = (encoder.num_planes, rows, cols)
    alphago_sl_policy = alphago_model(input_shape, is_policy_net=True)

    alphago_sl_policy.compile(loss='categorical_crossentropy', metrics=['accuracy'])
    # end::alphago_sl_model[]

    # tag::alphago_sl_train[]
    epochs = 4
    batch_size = 128
    alphago_sl_policy.fit_generator(
        generator=generator.generate(batch_size, num_classes),
        epochs=epochs,
        steps_per_epoch=generator.get_num_samples() / batch_size,
        validation_data=test_generator.generate(batch_size, num_classes),
        validation_steps=test_generator.get_num_samples() / batch_size,
        callbacks=[ModelCheckpoint('alphago_sl_policy_{epoch}.h5')]
    )

    alphago_sl_agent = DeepLearningAgent(alphago_sl_policy, encoder)

    with h5py.File('alphago_sl_policy.h5', 'w') as sl_agent_out:
        alphago_sl_agent.serialize(sl_agent_out)
    # end::alphago_sl_train[]

    alphago_sl_policy.evaluate_generator(
        generator=test_generator.generate(batch_size, num_classes),
        steps=test_generator.get_num_samples() / batch_size
    )


if __name__ == '__main__':
    freeze_support()
    main()
